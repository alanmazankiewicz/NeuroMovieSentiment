{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "# based on ideas of the paper 'Convolutional Neural Networks for Sentence Classification' by Yoon Kim (https://arxiv.org/abs/1408.5882)\n",
    "# and the following github code: https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext.data import TabularDataset\n",
    "from torchtext import data\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re \n",
    "import contractions\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "SEED = 1\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/Kaggle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINLEN = 100 # = maximum filter size\n",
    "\n",
    "def clean(text):\n",
    "    text = re.sub( '\\s+', ' ',  text).strip()  # remove duplicate whitespaces   \n",
    "    text = contractions.fix(text)  # replace contractions\n",
    "    return text\n",
    "\n",
    "def lemmatize(text):\n",
    "    doc = spacy_en(text)\n",
    "    lemmatized = \" \".join([token.lemma_ for token in doc])\n",
    "    return(lemmatized)\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = clean(text)  \n",
    "    # text = lemmatize(text) \n",
    "    tokens = [tok.text for tok in spacy_en.tokenizer(text)]  \n",
    "    if len(tokens) < MINLEN:    # pad each phrase to MINLEN so that all filters can be applied\n",
    "        tokens += ['<pad>'] * (MINLEN - len(tokens))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True)\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "\n",
    "# train_data = TabularDataset(f'{path}/train.tsv', 'tsv', fields=[('PhraseId', None),('SentenceId', None),('text', TEXT),('label', LABEL)], skip_header=True) # original train dataset\n",
    "train = TabularDataset(f'{path}/train_local.csv', 'csv', fields=[('PhraseId', None),('SentenceId', None),('text', TEXT),('label', LABEL)], skip_header=True) # local train dataset\n",
    "#train = TabularDataset(f'{path}/train_local_balanced.csv', 'csv', fields=[('PhraseId', None),('SentenceId', None),('text', TEXT),('label', LABEL)], skip_header=True) # local balanced train dataset\n",
    "\n",
    "validate = TabularDataset(f'{path}/valid_local.csv', 'csv', fields=[('PhraseId', None),('SentenceId', None),('text', TEXT),('label', LABEL)], skip_header=True) # local valid dataset\n",
    "\n",
    "# test_data = TabularDataset(f'{path}/test.tsv', 'tsv', fields=[('PhraseId', None),('SentenceId', None),('text', TEXT)], skip_header=True) # original test data\n",
    "test_data = TabularDataset(f'{path}/test_local.csv', 'csv', fields=[('PhraseId', None),('SentenceId', None),('text', TEXT)], skip_header=True) # local test dataset\n",
    "test_sentences = TabularDataset(f'{path}/test_local_sentences.csv', 'csv', fields=[('PhraseId', None),('SentenceId', None),('text', TEXT)], skip_header=True) # local test sentences\n",
    "\n",
    "# split train_data into train and validate\n",
    "# train, validate = train_data.split(split_ratio=0.8)  ## only for non-local!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, vectors=\"glove.6B.100d\")  \n",
    "LABEL.build_vocab(train)   # von train_data zu train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 11691\n",
      "Unique tokens in LABEL vocabulary: 5\n",
      "defaultdict(<function _default_unk_index at 0x0000025AEBB688C8>, {'2': 0, '3': 1, '1': 2, '4': 3, '0': 4})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n",
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    " \n",
    "valid_iter = BucketIterator(validate, batch_size=BATCH_SIZE, device=device, \n",
    "                          sort_key=lambda x: len(x.text), # how to group the data\n",
    "                          sort_within_batch=False, shuffle = True, repeat=False \n",
    ")\n",
    "\n",
    "train_iter = BucketIterator(train, batch_size=BATCH_SIZE, device=device, sort_key=lambda x: len(x.text), sort_within_batch=False,\n",
    " shuffle = True, repeat=False \n",
    ")\n",
    "\n",
    "test_iter = Iterator(test_data, batch_size=BATCH_SIZE, sort=False, device=device, sort_within_batch=False, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(fs,embedding_dim)) for fs in filter_sizes])\n",
    "        self.fc = nn.Linear(len(filter_sizes)*n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = [phrase length, batch size]        \n",
    "        x = x.permute(1, 0)                \n",
    "        # x = [batch size, phrase length]               \n",
    "        \n",
    "        #-----embedded = self.embedding(x)\n",
    "        embedded = self.dropout(self.embedding(x))  \n",
    "        # embedded = [batch size, phrase length, embedding dim]     \n",
    "        embedded = embedded.unsqueeze(1)  # unsqueeze(1) adds dimension of size 1 at index 1; to get required input shape for Conv2D\n",
    "        # embedded = [batch size, 1, phrase length, embedding dim]        \n",
    "       \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs] # squeeze(3) removes dimension of size 1 at index 3; W_out = 1 \n",
    "        # conv_n = [batch size, n_filters, phrase length - filter_sizes[n] + 1]        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]  \n",
    "        # len(pooled) = len(filter_sizes); for each filter_size: calculate for each of the n_filters filters the max value\n",
    "            # pooled_n = [batch size, n_filters]       \n",
    "        cat = torch.cat(pooled, dim=1)         \n",
    "        ##---cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        # cat = [batch size, n_filters * len(filter_sizes)]\n",
    "        return self.fc(cat)\n",
    "    \n",
    "    \n",
    "#----- Explainations concerning dimension:\n",
    "# Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "#   - input: (N,C_in,H,W) with  N=batch size, C_in=num of channels, H=height of input planes, W=width\n",
    "#   - output: (N, C_out, H_out, W_out), calculation of H_out and W_out see https://pytorch.org/docs/stable/nn.html\n",
    "#   - here: N=batch_size, C_out=n_filters, H_out = sent len - filter-height\n",
    "# F.relu(..) applies rectified linear unit function element-wise: F.relu(x)= max(0, x)\n",
    "# torch.nn.functional.max_pool1d(*args, **kwargs): Applies 1D max pooling over an input signal composed of several input planes.\n",
    "#   - here: max_pool1d(input, kernel_size); kernel_size = size of window to take a max over\n",
    "#   - output: [batch size, n_filters, 1] --> apply squeeze(2) to get the shape [batch size, n_filters]\n",
    "# torch.cat(tensors, dim=..): \n",
    "#  - Concatenates the given sequence of seq tensors in the given dimension. \n",
    "#  - All tensors must either have the same shape (except in the concatenating dimension) or be empty.\n",
    "# torch.nn.functional.dropout(input, p=0.5, training=True, inplace=False) \n",
    "#  - During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "FILTER_SIZES = [50,100] # [3,4,6,8] \n",
    "N_FILTERS = 120\n",
    "OUTPUT_DIM = 5\n",
    "DROPOUT = 0.6 \n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)\n",
    "\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()  #weight = torch.tensor([15, 1, 1, 1, 1]).float()  {'2': 0, '3': 1, '1': 2, '4': 3, '0': 4})\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(predictions):\n",
    "    max_idx = torch.max(predictions,1)[1]\n",
    "    return max_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label.long())\n",
    "        \n",
    "        pred_labels = get_labels(predictions).float()\n",
    "        acc = accuracy_score(batch.label, pred_labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm):\n",
    "    labels = LABEL.vocab.itos\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cm)\n",
    "    plt.title('Confusion matrix')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticklabels([''] + labels)\n",
    "    ax.set_yticklabels([''] + labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    conf_matrix = np.zeros((5,5))\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label.long())\n",
    "            \n",
    "            pred_labels = get_labels(predictions).float()        \n",
    "            acc = accuracy_score(batch.label, pred_labels)\n",
    "            \n",
    "            # conf_matrix += confusion_matrix(batch.label.long(), pred_labels, labels = [0,1,2,3,4])\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    \n",
    "    # print('conf_matrix: ', conf_matrix)\n",
    "    # plot_confusion_matrix(conf_matrix)\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 1.010 | Train Acc: 58.89% | Valid Loss: 1.010 | Valid Acc: 58.81% |\n",
      "| Epoch: 02 | Train Loss: 0.886 | Train Acc: 63.72% | Valid Loss: 0.983 | Valid Acc: 59.43% |\n",
      "| Epoch: 03 | Train Loss: 0.830 | Train Acc: 65.81% | Valid Loss: 0.961 | Valid Acc: 61.11% |\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 6\n",
    "\n",
    "loss = np.empty([2, EPOCHS])\n",
    "acc = np.empty([2, EPOCHS])\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iter, criterion)\n",
    "    \n",
    "    loss[0, epoch] = train_loss\n",
    "    loss[1, epoch] = valid_loss\n",
    "    acc[0, epoch] = train_acc\n",
    "    acc[1, epoch] = valid_acc\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Valid Loss: {valid_loss:.3f} | Valid Acc: {valid_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot(train, validate, ylabel, plotname, EPOCHS):\n",
    "    epochs = np.arange(EPOCHS)+1\n",
    "    plt.plot(epochs,train, 'b', label='train')\n",
    "    plt.plot(epochs,validate, 'g', label='validate')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend(shadow=False) \n",
    "    plt.savefig(plotname)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "     \n",
    "save_plot(loss[0,:], loss[1,:], 'loss', 'cnn_loss.png', EPOCHS)\n",
    "save_plot(acc[0,:], acc[1,:], 'accuracy','cnn_acc.png', EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_data):\n",
    "    prediction = torch.zeros(len(test_data))\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_data)):\n",
    "            test_sen = test_data[i].text\n",
    "            test_sen = [TEXT.vocab.stoi[x] for x in test_sen]\n",
    "            tensor = torch.LongTensor(test_sen).to(device)\n",
    "            tensor = tensor.unsqueeze(1) # tensor has shape [len(test_sen) x 1]\n",
    "            output = model(tensor)\n",
    "            out = F.softmax(output, 1)\n",
    "            pred_idx = torch.argmax(out[0]) \n",
    "            prediction[i] = int(LABEL.vocab.itos[pred_idx])\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub = pd.read_csv(f'{path}/sampleSubmission.csv')\n",
    "\n",
    "#sub.Sentiment = predict(model, test_data)\n",
    "#sub.Sentiment = sub.Sentiment.astype(int)\n",
    "#sub.to_csv('sub_cnn.csv', header = True, index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# local test data\n",
    "def predict_local(test_data_filename, test_data, model):\n",
    "    test_local = pd.read_csv(f'{path}/{test_data_filename}')\n",
    "    true = test_local.iloc[:,-1]\n",
    "    pred_local = predict(model, test_data)\n",
    "    acc = accuracy_score(true, pred_local)\n",
    "    print(acc)\n",
    "    return acc\n",
    "\n",
    "acc_test_local_sentences = predict_local('test_local_sentences.csv', test_sentences, model)\n",
    "acc_test_local = predict_local('test_local.csv', test_data, model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
