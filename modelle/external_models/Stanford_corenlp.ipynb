{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# precondition: download 'stanford-corenlp-full-2018-10-05', see https://stanfordnlp.github.io/CoreNLP/index.html#download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re \n",
    "import contractions\n",
    "\n",
    "from stanfordcorenlp import StanfordCoreNLP   # see https://stanfordnlp.github.io/CoreNLP/annotators.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set paths to data and stanfordcorenlp\n",
    "path = '../../data/Kaggle'\n",
    "stanfordcorenlp_path = '../../stanford-corenlp-full-2018-10-05' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_table(f'{path}/test.tsv')\n",
    "sub = pd.read_csv(f'{path}/sampleSubmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean data\n",
    "def clean(df, text_field):\n",
    "    df.loc[:,text_field] = df.loc[:,text_field].str.lower()   # lowercase    \n",
    "    \n",
    "    for i in range(df.shape[0]):\n",
    "        df.loc[i, text_field] = re.sub( '\\s+', ' ',  df.loc[i, text_field]).strip()   # remove duplicate whitespaces\n",
    "        df.loc[i, text_field] = contractions.fix(df.loc[i, text_field])     # replace contractions\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test = clean(test, 'Phrase')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[157451]\n",
      "      PhraseId  SentenceId Phrase\n",
      "1390    157451        8588      _\n"
     ]
    }
   ],
   "source": [
    "# cannot cope with whitespaces as input, therefore replace whitespaces by \"_\"\n",
    "def replace_whitespaces(data):\n",
    "    whitespace_phraseIDs = data.loc[data['Phrase'] == \" \"]['PhraseId'].ravel()   # if not cleaned change \"\" to \" \"\n",
    "    for i in whitespace_phraseIDs:\n",
    "        data.loc[data['PhraseId'] == i, 'Phrase'] = \"_\"\n",
    "        print(data.loc[data['PhraseId'] == i])\n",
    "    return data\n",
    "\n",
    "test = replace_whitespaces(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stanfordcorenlp_predict(data):\n",
    "    nlp = StanfordCoreNLP(f'{stanfordcorenlp_path}')\n",
    "\n",
    "    predicted = np.zeros(data.shape[0])\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        phrase = data.iloc[i]['Phrase']\n",
    "        res = nlp.annotate(phrase, properties={\n",
    "            \"annotators\": \"tokenize,ssplit,parse,sentiment\", # tokenizerAnnotator, WordsToSentencesAnnotator, ParserAnnotator, SentimentAnnotator\n",
    "            \"outputFormat\": \"json\",\n",
    "            # Only split the sentence at End Of Line. We assume that this method only takes in one single sentence.\n",
    "            \"ssplit.eolonly\": \"true\",\n",
    "            # Setting enforceRequirements to skip some annotators and make the process faster\n",
    "            \"enforceRequirements\": \"false\"\n",
    "        })\n",
    "        predicted[i] = res[res.find(\"sentimentValue\") + 18]   # not an optimal solution, but it works\n",
    "       \n",
    "    nlp.close() \n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub.Sentiment = stanfordcorenlp_predict(test)\n",
    "sub.Sentiment = sub.Sentiment.astype(int)\n",
    "\n",
    "sub.to_csv('sub_stanfordcorenlp.csv', header = True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# kaggle test score without cleaning: 0.645\n",
    "# kaggle test score with cleaning: 0.640 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
