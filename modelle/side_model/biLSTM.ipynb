{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi-directional LSTM\n",
    "# bases on ideas of: https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext.data import TabularDataset\n",
    "from torchtext import data\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "\n",
    "import pandas as pd\n",
    "import re \n",
    "import contractions\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from sklearn.metrics import confusion_matrix  \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/Kaggle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = re.sub( '\\s+', ' ',  text).strip()  # remove duplicate whitespaces   \n",
    "    text = contractions.fix(text)  # replace contractions\n",
    "    return text\n",
    "\n",
    "def lemmatize(text):\n",
    "    doc = spacy_en(text)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "def tokenizer(text): \n",
    "    text = clean(text) \n",
    "    # text = lemmatize(text)   \n",
    "    tokens = [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "    if (len(tokens) == 0):\n",
    "        tokens = ['.']\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 11691\n",
      "Unique tokens in LABEL vocabulary: 5\n",
      "defaultdict(<function _default_unk_index at 0x000001C4CF3C98C8>, {'2': 0, '3': 1, '1': 2, '4': 3, '0': 4})\n"
     ]
    }
   ],
   "source": [
    "TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True)\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "\n",
    "# train_data = TabularDataset(f'{path}/train.tsv', 'tsv', fields=[('PhraseId', None),('SentenceId', None),('text', TEXT),('label', LABEL)], skip_header=True) # original train dataset\n",
    "train = TabularDataset(f'{path}/train_local.csv', 'csv', fields=[('PhraseId', None),('SentenceId', None),('text', TEXT),('label', LABEL)], skip_header=True) # local train dataset\n",
    "#train = TabularDataset(f'{path}/train_local_balanced.csv', 'csv', fields=[('PhraseId', None),('SentenceId', None),('text', TEXT),('label', LABEL)], skip_header=True) # local balanced train dataset\n",
    "\n",
    "validate = TabularDataset(f'{path}/valid_local.csv', 'csv', fields=[('PhraseId', None),('SentenceId', None),('text', TEXT),('label', LABEL)], skip_header=True) # local valid dataset\n",
    "\n",
    "# test_data = TabularDataset(f'{path}/test.tsv', 'tsv', fields=[('PhraseId', None),('SentenceId', None),('text', TEXT)], skip_header=True) # original test data\n",
    "test_data = TabularDataset(f'{path}/test_local.csv', 'csv', fields=[('PhraseId', None),('SentenceId', None),('text', TEXT)], skip_header=True) # local test dataset\n",
    "test_sentences = TabularDataset(f'{path}/test_local_sentences.csv', 'csv', fields=[('PhraseId', None),('SentenceId', None),('text', TEXT)], skip_header=True) # local test sentences\n",
    "\n",
    "# split train_data into train and validate\n",
    "#---train, validate = train_data.split(split_ratio = 0.8) # only if not local!\n",
    " \n",
    "TEXT.build_vocab(train, vectors = \"glove.6B.100d\")  \n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n",
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    " \n",
    "valid_iter = BucketIterator(validate,batch_size=BATCH_SIZE, device=device, \n",
    "                          sort_key=lambda x: len(x.text), # how to group the data\n",
    "                          sort_within_batch=False, shuffle = True, repeat=False \n",
    ")\n",
    "\n",
    "train_iter = BucketIterator(train, batch_size=BATCH_SIZE, device=device, sort_key=lambda x: len(x.text), sort_within_batch=False,\n",
    " shuffle = True, repeat=False \n",
    ")\n",
    "\n",
    "test_iter = Iterator(test_data, batch_size=BATCH_SIZE, sort=False, device=device, sort_within_batch=False, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class biLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.biLstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_dim*2, output_dim)  # multipy with 2 because we use bidirectional lstm\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        #--- embedded = self.embedding(x)\n",
    "        output, (hidden, cell) = self.biLstm(embedded)   \n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        #---hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        return self.linear(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100 \n",
    "HIDDEN_DIM = 512 \n",
    "OUTPUT_DIM = 5\n",
    "NUM_LAYERS = 4 \n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.6 #0.5\n",
    "\n",
    "model = biLSTM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, NUM_LAYERS, BIDIRECTIONAL, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [-0.3552,  0.4732,  0.8660,  ...,  0.2483, -0.0049,  0.8731],\n",
       "        [-0.1431,  0.0487,  0.0565,  ..., -0.0402, -0.3744,  0.5650],\n",
       "        [ 0.1580, -0.2077,  0.0084,  ..., -1.2656, -0.2771, -0.3230]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')  #weight = torch.tensor([15, 2, 2,2,2]).float()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(predictions):\n",
    "    max_idx = torch.max(predictions,1)[1]\n",
    "    return max_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label.long())\n",
    "        \n",
    "        pred_labels = get_labels(predictions).float()\n",
    "        acc = accuracy_score(batch.label, pred_labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    conf_matrix = np.zeros((5,5))\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label.long())\n",
    "            \n",
    "            pred_labels = get_labels(predictions).float()        \n",
    "            acc = accuracy_score(batch.label, pred_labels)\n",
    "             \n",
    "            #conf_matrix += confusion_matrix(batch.label.long(), pred_labels, labels = [0,1,2,3,4])\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    # print('conf_matrix: ', conf_matrix)\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 32.744 | Train Acc: 57.96% | Valid Loss: 31.466 | Valid Acc: 59.76% |\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "loss = np.empty([2, EPOCHS])\n",
    "acc = np.empty([2, EPOCHS])\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iter, criterion)\n",
    "    \n",
    "    loss[0, epoch] = train_loss\n",
    "    loss[1, epoch] = valid_loss\n",
    "    acc[0, epoch] = train_acc\n",
    "    acc[1, epoch] = valid_acc\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Valid Loss: {valid_loss:.3f} | Valid Acc: {valid_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot(train, validate, ylabel, plotname, EPOCHS):\n",
    "    epochs = np.arange(EPOCHS)+1\n",
    "    plt.plot(epochs,train, 'b', label='train')\n",
    "    plt.plot(epochs,validate, 'g', label='validate')\n",
    "    \n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend(shadow=False)\n",
    "    plt.savefig(plotname)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "save_plot(loss[0,:], loss[1,:], 'loss', 'BLSTM_loss.png', EPOCHS)\n",
    "save_plot(acc[0,:], acc[1,:], 'accuracy','BLSTM_acc.png', EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_data):\n",
    "    prediction = torch.zeros(len(test_data))\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_data)):\n",
    "            test_sen = test_data[i].text\n",
    "            test_sen = [TEXT.vocab.stoi[x] for x in test_sen]\n",
    "            tensor = torch.LongTensor(test_sen).to(device)\n",
    "            tensor = tensor.unsqueeze(1) # tensor has shape [len(test_sen) x 1]\n",
    "            output = model(tensor)\n",
    "            out = F.softmax(output, 1)\n",
    "            pred_idx = torch.argmax(out[0]) \n",
    "            prediction[i] = int(LABEL.vocab.itos[pred_idx])\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub = pd.read_csv(f'{path}/sampleSubmission.csv')\n",
    "\n",
    "#sub.Sentiment = predict(model, test_data)\n",
    "#sub.Sentiment = sub.Sentiment.astype(int)\n",
    "#sub.to_csv('sub_blstm.csv', header = True, index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local test data\n",
    "def predict_local(test_filename, test_data, model):\n",
    "    test_local = pd.read_csv(f'{path}/{test_filename}')\n",
    "    true = test_local.iloc[:,-1]\n",
    "    pred_local = predict(model, test_data)\n",
    "    acc = accuracy_score(true, pred_local)\n",
    "    print(acc)\n",
    "    return acc\n",
    "\n",
    "acc_test_local_sentences = predict_local('test_local_sentences.csv', test_sentences, model)\n",
    "acc_test_local = predict_local('test_local.csv',test_data, model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
